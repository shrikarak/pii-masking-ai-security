{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Data Security: Automated PII Detection and Masking\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady. All rights reserved.**\n",
    "\n",
    "This notebook demonstrates a powerful technique for improving data security: using a Natural Language Processing (NLP) model to automatically detect and redact Personally Identifiable Information (PII) from text. We will use the `spaCy` library, combining a pre-trained Named Entity Recognition (NER) model with custom rules to build a robust PII detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation\n",
    "First, we need to install `spaCy` and download its small English language model, which includes a pre-trained NER component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "\n",
    "print(\"spaCy version:\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the PII Detection Pipeline\n",
    "\n",
    "Our strategy is a hybrid one:\n",
    "1.  Use spaCy's pre-trained NER model to find common entities like people's names (`PERSON`) and locations (`GPE`).\n",
    "2.  Add a custom `EntityRuler` to explicitly find patterns that the pre-trained model might miss, such as email addresses and phone numbers, using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Original pipeline components:\", nlp.pipe_names)\n",
    "\n",
    "# Define custom patterns for the EntityRuler\n",
    "patterns = [\n",
    "    {\"label\": \"EMAIL\", \"pattern\": [{\\"TEXT\": {\"REGEX\": \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"}}], \"id\": \"email_address\"},\n",
    "    {\"label\": \"PHONE\", \"pattern\": [{\\"TEXT\": {\"REGEX\": \"\\\\(?\\\\d{3}\\\\)?[\\\\s.-]?\\\\d{3}[\\\\s.-]?\\\\d{4}\"}}], \"id\": \"phone_number\"},\n",
    "    {\"label\": \"CREDIT_CARD\", \"pattern\": [{\\"TEXT\": {\"REGEX\": \"\\\\b\\\\d{4}[-\\\\s]?\\\\d{4}[-\\\\s]?\\\\d{4}[-\\\\s]?\\\\d{4}\\\\b\"}}], \"id\": \"credit_card\"}\n",
    "]\n",
    "\n",
    "# Create the EntityRuler and add it to the pipeline *before* the main NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "print(\"\\nUpdated pipeline components:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Masking Logic\n",
    "\n",
    "Now we create a function that takes raw text, processes it with our pipeline, and returns a redacted version of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pii(text):\n",
    "    \"\"\"Processes text with our NLP pipeline and masks detected PII entities.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create a list of PII entities to mask\n",
    "    pii_entities = []\n",
    "    for ent in doc.ents:\n",
    "        # The model might flag things that aren't truly sensitive. We specify which labels to mask.\n",
    "        if ent.label_ in [\"PERSON\", \"GPE\", \"EMAIL\", \"PHONE\", \"CREDIT_CARD\"]:\n",
    "            pii_entities.append(ent)\n",
    "    \n",
    "    # Reverse the list of entities to avoid index issues when replacing text\n",
    "    pii_entities.reverse()\n",
    "    \n",
    "    masked_text = text\n",
    "    for ent in pii_entities:\n",
    "        masked_text = masked_text[:ent.start_char] + f\"[REDACTED_{ent.label_}]\" + masked_text[ent.end_char:]\n",
    "    \n",
    "    return masked_text, pii_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the Demonstration\n",
    "\n",
    "Let's test our pipeline with a sample text containing various types of PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Hello, my name is John Doe and I live in New York. \n",
    "My colleague, Jane Smith from London, will also be on the call. \n",
    "You can reach me at john.doe@email.com or on my cell at (123) 456-7890. \n",
    "Please charge the payment to my card 1234-5678-9012-3456. \n",
    "My old address was in San Francisco.\n",
    "\"\"\"\n",
    "\n",
    "# Get the masked text and the list of detected entities\n",
    "masked_output, detected_entities = mask_pii(sample_text)\n",
    "\n",
    "print(\"--- ORIGINAL TEXT ---\\n\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"--- DETECTED PII ENTITIES ---\\n\")\n",
    "if detected_entities:\n",
    "    for ent in sorted(detected_entities, key=lambda x: x.start_char):\n",
    "        print(f\"- Found '{ent.text}' (Label: {ent.label_})\")\n",
    "else:\n",
    "    print(\"No PII entities were found.\")\n",
    "\n",
    "print(\"\\n--- MASKED TEXT ---\\n\")\n",
    "print(masked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
